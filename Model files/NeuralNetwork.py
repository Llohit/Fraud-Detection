# -*- coding: utf-8 -*-
"""neural.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nH35Lu5MAEf2DKycUf_ljpiMoLkJA1dQ
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

newdf=pd.read_csv('../input/fraudaa/newdf2.csv')

newtest=pd.read_csv('../input/fraudaa/newtest.csv')

dte=pd.read_csv("../input/its-a-fraud/test.csv")

ar=['dist2', 'R_emaildomain', 'D6', 'D7', 'D8', 'D9', 'D12', 'D13', 'D14', 'V138', 'V139', 'V140', 'V141', 'V142', 'V143', 'V144', 'V145', 'V146', 'V147', 'V148', 'V149', 'V150', 'V151', 'V152', 'V153', 'V154', 'V155', 'V156', 'V157', 'V158', 'V159', 'V160', 'V161', 'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V168', 'V169', 'V170', 'V171', 'V172', 'V173', 'V174', 'V175', 'V176', 'V177', 'V178', 'V179', 'V180', 'V181', 'V182', 'V183', 'V184', 'V185', 'V186', 'V187', 'V188', 'V189', 'V190', 'V191', 'V192', 'V193', 'V194', 'V195', 'V196', 'V197', 'V198', 'V199', 'V200', 'V201', 'V202', 'V203', 'V204', 'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V211', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V218', 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234', 'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278', 'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330', 'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338', 'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']

ar1=[]
for i in newdf.columns:
    if(i in ar):
        ar1.append(i)

for i in newdf.columns:
    if(i in ar1):
        newdf.drop(i,axis=1,inplace=True)

for i in newtest.columns:
    if(i in ar1):
        newtest.drop(i,axis=1,inplace=True)

X_train=newdf.drop(['isFraud','TransactionID'],axis=1,inplace=False)
y_train=newdf['isFraud']

X_test=newtest.copy()

X_test=newtest.drop(['TransactionID'],axis=1,inplace=False)

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

#over = SMOTE(sampling_strategy=0.1)
#under = RandomUnderSampler(sampling_strategy=0.5)
rus = RandomUnderSampler(0.4,random_state=42)
#steps = [('o', over), ('u', under)]
#pipeline = Pipeline(steps=steps)

X_res, y_res = rus.fit_resample(X_train, y_train)

#smote = SMOTE(0.30,random_state=42)
#X_res, y_res =  smote.fit_resample(X_train,y_train)
#X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.25, random_state = 42)

from collections import Counter 
Counter(y_res)

mlp_gs = MLPClassifier(max_iter=100)
parameter_space = {
    'hidden_layer_sizes': [(10,30,10),(20,)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}
from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)
clf.fit(X, y) # X is train samples and y is the corresponding labels

from sklearn.neural_network import MLPClassifier
model= MLPClassifier(hidden_layer_sizes=(100,))
model.fit(X_res, y_res)

y_pred=model.predict(X_test)

Counter(y_pred)

y_pred1

newtest['IsFraud']=y_pred

neww=newtest[['TransactionID','IsFraud']]

dte = dte.merge(neww, how='outer',copy=False, on ='TransactionID' )

yp=dte['IsFraud']

Counter(yp)

yp.to_csv('ypred56.csv')

model.predict(X_test)

Counter(model.predict(X_test))

import tensorflow
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

print(len(X_train.columns))

# define model
model = Sequential()
# define first hidden layer and visible layer
model.add(Dense(50, input_dim=117, activation='relu', kernel_initializer='he_uniform'))
# define output layer
model.add(Dense(1, activation='sigmoid'))
# define loss and optimizer
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit(X_train,y_train,epochs=10)

y_pred1=model.predict(X_test)

len(y_pred1)

weights_assigned={0:1,1:30}
model = Sequential()
# define first hidden layer and visible layer
model.add(Dense(50, input_dim=117, activation='relu', kernel_initializer='he_uniform'))
# define output layer
model.add(Dense(1, activation='sigmoid'))
# define loss and optimizer
model.compile(loss='binary_crossentropy', optimizer='adam')
model.fit(X_train,y_train,class_weight=weights_assigned,epochs=10)

y_pred1=model.predict(X_test)

print(y_pred1)

c1=0
c0=0
s=set()
d={}
for i in y_pred1:
    if(i[0] not in d):
        d[i[0]]=1
    else:
        d[i[0]]+=1
    if(i[0]>0.4):
        c1+=1
    else:
        c0+=1
print(c1,c0)

from collections import Counter
a=[]
for i in y_pred1:
    if(i[0]>0.80):
        a.append(1)
    else:
        a.append(0)
Counter(a)

print(y_pred1)

